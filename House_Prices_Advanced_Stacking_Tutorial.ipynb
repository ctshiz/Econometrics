{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ctshiz/Econometrics/blob/main/House_Prices_Advanced_Stacking_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "z2JAJaGdnm-o"
      },
      "cell_type": "markdown",
      "source": [
        "## **0. Introduction**"
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "b7NBzlPgnm-u"
      },
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.max_rows', 200)\n",
        "from scipy.stats import probplot\n",
        "from scipy.special import boxcox1p\n",
        "from scipy.stats import boxcox_normmax\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from mlxtend.regressor import StackingCVRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "SEED = 42\n",
        "PATH = '../input/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "9mjEQVm8nm-y"
      },
      "cell_type": "code",
      "source": [
        "def concat_df(train_data, test_data):\n",
        "    # Returns a concatenated df of training and test set on axis 0\n",
        "    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n",
        "\n",
        "def divide_df(all_data):\n",
        "    # Returns divided dfs of training and test set\n",
        "    return all_data.loc[:1459], all_data.loc[1460:].drop(['SalePrice'], axis=1)\n",
        "\n",
        "df_train = pd.read_csv(PATH + 'train.csv')\n",
        "df_test = pd.read_csv(PATH + 'test.csv')\n",
        "df_all = concat_df(df_train, df_test)\n",
        "\n",
        "df_train.name = 'Training Set'\n",
        "df_test.name = 'Test Set'\n",
        "df_all.name = 'All Set'\n",
        "\n",
        "dfs = [df_train, df_test]\n",
        "\n",
        "print('Number of Training Examples = {}'.format(df_train.shape[0]))\n",
        "print('Number of Test Examples = {}\\n'.format(df_test.shape[0]))\n",
        "print('Training X Shape = {}'.format(df_train.shape))\n",
        "print('Training y Shape = {}\\n'.format(df_train['SalePrice'].shape[0]))\n",
        "print('Test X Shape = {}'.format(df_test.shape))\n",
        "print('Test y Shape = {}\\n'.format(df_test.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TRcrjms1nm-0"
      },
      "cell_type": "markdown",
      "source": [
        "## **1. Exploratory Data Analysis**"
      ]
    },
    {
      "metadata": {
        "id": "calmCTaDnm-0"
      },
      "cell_type": "markdown",
      "source": [
        "### **1.1 Overview**\n",
        "* Training set has **1460** samples and test set has **1459** samples\n",
        "* Training set have **81** features and test set have **80** features\n",
        "* One extra feature in training set is `SalePrice` which is the target to predict"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "bIuw-iKdnm-1"
      },
      "cell_type": "code",
      "source": [
        "print(df_all.info())\n",
        "df_all.sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b7HHVF5onm-2"
      },
      "cell_type": "markdown",
      "source": [
        "### **1.2 Missing Values**\n",
        "There are many features with missing values in both training set and test set. `display_missing` function shows the count of missing values in every feature if they have at least 1 missing value.\n",
        "* Training set has missing values in **19** features\n",
        "* Test set has missing values in **32** features\n",
        "\n",
        "There are two types of missing data in this dataset; **systematically** missing data and **randomly** missing data. Majority of the missing data are **systematically** missing which is easier to fix, but **randomly** missing data requires extra effort. It is better to deal with those types of missing data separately."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "IgCklXognm-3"
      },
      "cell_type": "code",
      "source": [
        "def display_missing(df):\n",
        "    for col in df.columns.tolist():\n",
        "        if df[col].isnull().sum():\n",
        "            print('{} column missing values: {}/{}'.format(col, df[col].isnull().sum(), len(df)))\n",
        "    print('\\n')\n",
        "\n",
        "for df in dfs:\n",
        "    print('{}'.format(df.name))\n",
        "    display_missing(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jUK-NJSenm-4"
      },
      "cell_type": "markdown",
      "source": [
        "#### **1.2.1 Systematically Missing Data**\n",
        "`MasVnrArea` and `MasVnrType` are defined as masonry veneer area in square feet and masonry veneer type. Missing values in those features mean that there is no masonry veneer in those houses. Missing values in `MasVnrArea` are filled with **0** and missing values in `MasVnrType` are filled with **None**.\n",
        "\n",
        "`BsmtFinSF1`, `BsmtFinSF2`, `BsmtUnfSF` and `TotalBsmtSF` are basement area in square feet. There is only **1** missing value in those features and it is the same house. That house has missing values in other basement features as well and it most likely doesn't have basement. That's why missing values in `BsmtFinSF1`, `BsmtFinSF2`, `BsmtUnfSF` and `TotalBsmtSF` are filled with **0**. The other basement features are categorical. `BsmtCond`, `BsmtExposure`, `BsmtFinType1`, `BsmtFinType2` and `BsmtQual` are missing in houses without basements, so they are filled with **None**. There are also ordinal basement features with missing values such as `BsmtFullBath` and `BsmtHalfBath`. Missing values in those features are filled with **0**.\n",
        "\n",
        "There are **7** garage features and all of them have missing values. `GarageArea` is defined as garage area in square feet. There is only **1** house with missing `GarageArea`. That house also has missing value in `GarageCars` feature. Both `GarageArea` and `GarageCars` features are filled with **0** because that house has no garage. `GarageType`, `GarageYrBlt`, `GarageFinish`, `GarageQual` and `GarageCond` are categorical garage features. All of them except `GarageYrBlt` are filled with **None**. `GarageYrBlt` is filled with **0** because it is a numerical type.\n",
        "\n",
        "Other systematically missing categorical features are `Alley`, `Fence`, `FireplaceQu`, `MiscFeature` and `PoolQC`. There are missing values in them because those features doesn't exist in those houses. Missing values in those features are also filled with **None**."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "QbT8bgLUnm-4"
      },
      "cell_type": "code",
      "source": [
        "# Filling masonry veneer features\n",
        "df_all['MasVnrArea'] = df_all['MasVnrArea'].fillna(0)\n",
        "df_all['MasVnrType'] = df_all['MasVnrType'].fillna('None')\n",
        "\n",
        "# Filling continuous basement features\n",
        "for feature in ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']:\n",
        "    df_all[feature] = df_all[feature].fillna(0)\n",
        "\n",
        "# Filling categorical basement features\n",
        "for feature in ['BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual']:\n",
        "    df_all[feature] = df_all[feature].fillna('None')\n",
        "\n",
        "# Filling continuous garage features\n",
        "for feature in ['GarageArea', 'GarageCars', 'GarageYrBlt']:\n",
        "    df_all[feature] = df_all[feature].fillna(0)\n",
        "\n",
        "# Filling categorical garage features\n",
        "for feature in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n",
        "    df_all[feature] = df_all[feature].fillna('None')\n",
        "\n",
        "# Filling other categorical features\n",
        "for feature in ['Alley', 'Fence', 'FireplaceQu', 'MiscFeature', 'PoolQC']:\n",
        "    df_all[feature] = df_all[feature].fillna('None')\n",
        "\n",
        "display_missing(df_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Ar2onp5nm-5"
      },
      "cell_type": "markdown",
      "source": [
        "#### **1.2.2 Randomly Missing Data**\n",
        "After filling systematically missing data, there are few features left with missing values. Those are the remaining randomly missing data. A house can't exist without those features, so they can't be filled with **0** or **None**. The amount of randomly missing data in a feature is also smaller which makes it possible to fill them with descriptive statistical measures. The statistical measures are based on the groups of neighborhoods and building classes because a house would most likely look like its neighbors.\n",
        "*  `Electrical`, `Exterior1st`, `Exterior2nd`, `Functional`, `KitchenQual`, `MSZoning`, `SaleType` and `Utilities` are categorical features with randomly missing data and missing values in those features are filled with mode of building class and neighborhood groups\n",
        "* `LotFrontage` is the only randomly missing continuous feature and missing values in `LotFrontage` are filled with the median values of neighborhoods"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "m-czj5_Unm-6"
      },
      "cell_type": "code",
      "source": [
        "# Filling missing values in categorical features with the mode value of neighborhood and house type\n",
        "for feature in ['Electrical', 'Exterior1st', 'Exterior2nd', 'Functional', 'KitchenQual', 'MSZoning', 'SaleType', 'Utilities']:\n",
        "    df_all[feature] = df_all.groupby(['Neighborhood', 'MSSubClass'])[feature].apply(lambda x: x.fillna(x.mode()[0]))\n",
        "\n",
        "# Filling the missing values in LotFrontage with the median of neighborhood\n",
        "df_all['LotFrontage'] = df_all.groupby(['Neighborhood'])['LotFrontage'].apply(lambda x: x.fillna(x.median()))\n",
        "\n",
        "display_missing(df_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KUBSwAP2nm-6"
      },
      "cell_type": "markdown",
      "source": [
        "### **1.3 Target Distribution**\n",
        "Target is not normally distributed. Training set `SalePrice` skew is **1.88** which clearly shows that the target has high positive skew. The distribution is asymmetrical because of the extremely high outliers.\n",
        "\n",
        "Training set `SalePrice` kurtosis is **6.53** which is an indicator of the tail extremity. Mean `SalePrice` is **180921.2**, however, median is **163000**. This huge gap between mean and median is also the effect of outliers.\n",
        "\n",
        "Probability plot clearly illustrates that outliers will strongly affect the regression models since a single outlier may result in all predictor coefficients being biased. The probability being a convex curve rather than a straight line is the result of the skewness."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "zfV1Cp8cnm-7"
      },
      "cell_type": "code",
      "source": [
        "print('Training Set SalePrice Skew: {}'.format(df_train['SalePrice'].skew()))\n",
        "print('Training Set SalePrice Kurtosis: {}'.format(df_train['SalePrice'].kurt()))\n",
        "print('Training Set SalePrice Mean: {}'.format(df_train['SalePrice'].mean()))\n",
        "print('Training Set SalePrice Median: {}'.format(df_train['SalePrice'].median()))\n",
        "print('Training Set SalePrice Max: {}'.format(df_train['SalePrice'].max()))\n",
        "\n",
        "fig, axs = plt.subplots(nrows=2, figsize=(16, 16))\n",
        "plt.subplots_adjust(left=None, bottom=5, right=None, top=6, wspace=None, hspace=None)\n",
        "\n",
        "sns.distplot(df_train['SalePrice'], hist=True, ax=axs[0])\n",
        "probplot(df_train['SalePrice'], plot=axs[1])\n",
        "\n",
        "axs[0].set_xlabel('Sale Price', size=12.5, labelpad=12.5)\n",
        "axs[1].set_xlabel('Theoretical Quantiles', size=12.5, labelpad=12.5)\n",
        "axs[1].set_ylabel('Ordered Values', size=12.5, labelpad=12.5)\n",
        "\n",
        "for i in range(2):\n",
        "    axs[i].tick_params(axis='x', labelsize=12.5)\n",
        "    axs[i].tick_params(axis='y', labelsize=12.5)\n",
        "\n",
        "axs[0].set_title('Distribution of Sale Price in Training Set', size=15, y=1.05)\n",
        "axs[1].set_title('Sale Price Probability Plot', size=15, y=1.05)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Jw7AkMgnm-7"
      },
      "cell_type": "markdown",
      "source": [
        "### **1.4 Correlations**\n",
        "Features are strongly correlated with target. **8** features have more than **0.3** correlation coefficient with `SalePrice` and **3** of them are higher than **0.6**. It looks like multicollinearity occurs in the dataset.\n",
        "\n",
        "The other features are also strongly correlated with each other and dependent to each other. There are more than **25** correlations with at least **0.5** coefficient. The highest among them is between `GarageArea` and `GarageCars` which is **0.88**.\n",
        "\n",
        "The correlation coefficients of training set and test set are very close."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "QPl-Uvnxnm-8"
      },
      "cell_type": "code",
      "source": [
        "df_train, df_test = divide_df(df_all)\n",
        "# Dropping categorical features\n",
        "cols = ['GarageYrBlt', 'Id', 'MSSubClass', 'MoSold', 'YearBuilt', 'YearRemodAdd', 'YrSold']\n",
        "\n",
        "df_train_corr = df_train.drop(cols, axis=1).corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n",
        "df_train_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\n",
        "df_train_corr.drop(df_train_corr.iloc[1::2].index, inplace=True)\n",
        "df_train_corr_nd = df_train_corr.drop(df_train_corr[df_train_corr['Correlation Coefficient'] == 1.0].index)\n",
        "\n",
        "df_test_corr = df_test.drop(cols, axis=1).corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n",
        "df_test_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\n",
        "df_test_corr.drop(df_test_corr.iloc[1::2].index, inplace=True)\n",
        "df_test_corr_nd = df_test_corr.drop(df_test_corr[df_test_corr['Correlation Coefficient'] == 1.0].index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "RbGiwGO3nm-9"
      },
      "cell_type": "code",
      "source": [
        "# Features correlated with target\n",
        "df_train_corr_nd[df_train_corr_nd['Feature 1'] == 'SalePrice']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "6BXRL3jGnm-9"
      },
      "cell_type": "code",
      "source": [
        "# Training set high correlations\n",
        "df_train_corr_nd.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "Ohg5VRyknm--"
      },
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(nrows=2, figsize=(50, 50))\n",
        "\n",
        "sns.heatmap(df_train.drop(cols, axis=1).corr().round(2), ax=axs[0], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 12})\n",
        "sns.heatmap(df_test.drop(cols, axis=1).corr().round(2), ax=axs[1], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 12})\n",
        "\n",
        "for i in range(2):\n",
        "    axs[i].tick_params(axis='x', labelsize=13)\n",
        "    axs[i].tick_params(axis='y', labelsize=13)\n",
        "\n",
        "axs[0].set_title('Training Set Correlations', size=15)\n",
        "axs[1].set_title('Test Set Correlations', size=15)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vmSOm0renm--"
      },
      "cell_type": "markdown",
      "source": [
        "### **1.5 Target vs Features**"
      ]
    },
    {
      "metadata": {
        "id": "NgdtHcMznm--"
      },
      "cell_type": "markdown",
      "source": [
        "#### **1.5.1 Numerical Features**\n",
        "Data points of `1stFlrSF`, `BsmtUnfSF`, `GarageArea`, `GrLivArea`, `LotArea`, `LotFrontage` and `TotalBsmtSF` features are not stacked at **0** as much as other numerical features. Those features exist in almost every single house. Fitting the regression line is easier for those features. In addition to that, outliers are very visible in those features.\n",
        "\n",
        "Data points of `2ndFlrSF`, `3SsnPorch`, `BsmtFinSF1`, `BsmtFinSF2`, `EnclosedPorch`, `LowQualFinSF`, `MasVnrArea`, `MiscVal`, `OpenPorchSF`, `PoolArea`, `ScreenPorch` and `WoodDeckSF` features are heavily stacked at **0**. Those features are rarer than the previous ones and they don't exist in every house, so they are sparse features. Those sparse features may not be reliable as the previous features when they are used as continuous features, because they are going to introduce bias to the regression function.\n",
        "\n",
        "`GarageYrBlt`, `YearBuilt` and `YearRemodAdd` are ordinal features, but a linear relationship can be seen from their plots. Houses with recent dates are more likely to be sold at higher prices."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "63nMzcbGnm--"
      },
      "cell_type": "code",
      "source": [
        "num_features = ['1stFlrSF', '2ndFlrSF', '3SsnPorch', 'BsmtFinSF1', 'BsmtFinSF2',\n",
        "                'BsmtUnfSF', 'EnclosedPorch', 'GarageArea', 'GarageYrBlt', 'GrLivArea',\n",
        "                'LotArea', 'LotFrontage', 'LowQualFinSF', 'MasVnrArea', 'MiscVal',\n",
        "                'OpenPorchSF', 'PoolArea', 'ScreenPorch', 'TotalBsmtSF', 'WoodDeckSF',\n",
        "                'YearBuilt', 'YearRemodAdd']\n",
        "\n",
        "fig, axs = plt.subplots(ncols=2, nrows=11, figsize=(12, 80))\n",
        "plt.subplots_adjust(right=1.5)\n",
        "cmap = sns.cubehelix_palette(dark=0.3, light=0.8, as_cmap=True)\n",
        "\n",
        "for i, feature in enumerate(num_features, 1):\n",
        "    plt.subplot(11, 2, i)\n",
        "    sns.scatterplot(x=feature, y='SalePrice', hue='SalePrice', size='SalePrice', palette=cmap, data=df_train)\n",
        "\n",
        "    plt.xlabel('{}'.format(feature), size=15)\n",
        "    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n",
        "\n",
        "    for j in range(2):\n",
        "        plt.tick_params(axis='x', labelsize=12)\n",
        "        plt.tick_params(axis='y', labelsize=12)\n",
        "\n",
        "    plt.legend(loc='best', prop={'size': 12})\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WoXxEvEfnm-_"
      },
      "cell_type": "markdown",
      "source": [
        "#### **1.5.2 Categorical Features**\n",
        "Categorical features are not strongly correlated with `SalePrice`. There are only **2** categorical features that have significant correlation with `SalePrice`, and they are `OverallQual` and `TotRmsAbvGrd`. A linear relationship can easily be seen from their plots. When the number of `OverallQual` and `TotRmsAbvGrd` increases, `SalePrice` tends to increase as well.\n",
        "\n",
        "Data points of `MoSold` and `YrSold` are uniformly distributed between classes. Those two features might have the least information about `SalePrice` among other categorical features.\n",
        "\n",
        "The other categorical features don't have significant correlation with `SalePrice`. However, values in some of those features have very distinct `SalePrice` maximums, minimums and interquartile ranges. Those features could be useful in tree based algorithms.  "
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "Pvwlz4yTnm-_"
      },
      "cell_type": "code",
      "source": [
        "cat_features = ['Alley', 'BedroomAbvGr', 'BldgType', 'BsmtCond', 'BsmtExposure',\n",
        "                'BsmtFinType1', 'BsmtFinType2', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtQual',\n",
        "                'CentralAir', 'Condition1', 'Condition2', 'Electrical', 'ExterCond',\n",
        "                'ExterQual', 'Exterior1st', 'Exterior2nd', 'Fence', 'FireplaceQu',\n",
        "                'Fireplaces', 'Foundation', 'FullBath', 'Functional', 'GarageCars',\n",
        "                'GarageCond', 'GarageFinish', 'GarageQual', 'GarageType', 'HalfBath',\n",
        "                'Heating', 'HeatingQC', 'KitchenAbvGr', 'KitchenQual', 'LandContour',\n",
        "                'LandSlope', 'LotConfig', 'LotShape', 'MSSubClass', 'MSZoning',\n",
        "                'MasVnrType', 'MiscFeature', 'MoSold', 'Neighborhood', 'OverallCond',\n",
        "                'OverallQual', 'PavedDrive', 'PoolQC', 'RoofMatl', 'RoofStyle',\n",
        "                'SaleCondition', 'SaleType', 'Street', 'TotRmsAbvGrd', 'Utilities', 'YrSold']\n",
        "\n",
        "fig, axs = plt.subplots(ncols=2, nrows=28, figsize=(18, 120))\n",
        "plt.subplots_adjust(right=1.5, top=1.5)\n",
        "\n",
        "for i, feature in enumerate(cat_features, 1):\n",
        "    plt.subplot(28, 2, i)\n",
        "    sns.swarmplot(x=feature, y='SalePrice', data=df_train, palette='Set3')\n",
        "\n",
        "    plt.xlabel('{}'.format(feature), size=25)\n",
        "    plt.ylabel('SalePrice', size=25, labelpad=15)\n",
        "\n",
        "    for j in range(2):\n",
        "        if df_train[feature].value_counts().shape[0] > 10:\n",
        "            plt.tick_params(axis='x', labelsize=7)\n",
        "        else:\n",
        "            plt.tick_params(axis='x', labelsize=20)\n",
        "        plt.tick_params(axis='y', labelsize=20)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SI3ehjo8nm-_"
      },
      "cell_type": "markdown",
      "source": [
        "### **1.6 Training vs Test Set**"
      ]
    },
    {
      "metadata": {
        "id": "_pr4Q1oBnm_A"
      },
      "cell_type": "markdown",
      "source": [
        "#### **1.6.1 Numerical Features**\n",
        "`1stFloorSF`, `2ndFloorSF`, `BsmtFinSF1`, `BsmtUnfSF`, `GarageArea`, `GarageYrBlt`, `GrLivArea`, `LotArea`, `LotFrontage`, `MasVnrArea`, `OpenPorchSF`, `TotalBsmtSF`, `WoodDeckSF`, `YearBuilt` and `YearRemodAdd` features have similar distributions in training set and test set. Models using these features, are less likely to overfit.\n",
        "\n",
        "`3SsnPorch`, `BsmtFinSF2`, `EnclosedPorch`, `LowQualFinSF`, `MiscVal`, `PoolArea` and `ScreenPorch` are too noisy. The distributions of those features in training set and test set doesn't match, so they might introduce bias to the models."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "syUSx3JSnm_A"
      },
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(ncols=2, nrows=11, figsize=(12, 80))\n",
        "plt.subplots_adjust(right=1.5)\n",
        "\n",
        "for i, feature in enumerate(num_features, 1):\n",
        "    plt.subplot(11, 2, i)\n",
        "    sns.kdeplot(df_train[feature], bw='silverman', label='Training Set', shade=True)\n",
        "    sns.kdeplot(df_test[feature], bw='silverman', label='Test Set', shade=True)\n",
        "\n",
        "    plt.xlabel('{}'.format(feature), size=15)\n",
        "\n",
        "    for j in range(2):\n",
        "        plt.tick_params(axis='x', labelsize=12)\n",
        "        plt.tick_params(axis='y', labelsize=12)\n",
        "\n",
        "    plt.legend(loc='best', prop={'size': 15})\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r2vZ2h08nm_B"
      },
      "cell_type": "markdown",
      "source": [
        "#### **1.6.2 Categorical Features**\n",
        "Values in some categorical features exist in one set, but doesn't exist in another set. This problem exists in `BedroomAbvGr`, `Condition2`, `Electrical`, `Exterior1st`, `Exterior2nd`, `Fireplaces`, `FullBath`, `GarageCars`, `GarageQual`, `Heating`, `KitchenAbvGr`, `MSSubClass`, `MiscFeature`, `PoolQC`, `RoofMatl` and `Utilities` features. This is a potential problem because some of the categorical features are going to be one-hot encoded, and because of that process, the feature counts of training set and test set might not match.\n",
        "\n",
        "Other categorical feature value counts in training set and test set are close to each other. Those features have similar distributions in both data sets, so they are more reliable than the previous ones."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "ePysMQRtnm_B"
      },
      "cell_type": "code",
      "source": [
        "df_train['Dataset'] = 'Training Set'\n",
        "df_test['Dataset'] = 'Test Set'\n",
        "df_all = concat_df(df_train, df_test)\n",
        "\n",
        "fig, axs = plt.subplots(ncols=2, nrows=28, figsize=(18, 120))\n",
        "plt.subplots_adjust(right=1.5, top=1.5)\n",
        "\n",
        "for i, feature in enumerate(cat_features, 1):\n",
        "    plt.subplot(28, 2, i)\n",
        "    sns.countplot(x=feature, hue='Dataset', data=df_all, palette='Set2')\n",
        "\n",
        "    plt.xlabel('{}'.format(feature), size=25)\n",
        "    plt.ylabel('Count', size=25)\n",
        "\n",
        "    for j in range(2):\n",
        "        if df_train[feature].value_counts().shape[0] > 10:\n",
        "            plt.tick_params(axis='x', labelsize=7)\n",
        "        else:\n",
        "            plt.tick_params(axis='x', labelsize=20)\n",
        "        plt.tick_params(axis='y', labelsize=20)\n",
        "\n",
        "    plt.legend(loc='upper right', prop={'size': 15})\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "waIM_636nm_C"
      },
      "cell_type": "markdown",
      "source": [
        "### **1.7 Conclusion**\n",
        "There are features with ambiguous types. `GarageYrBlt`, `MoSold`, `YearBuilt`, `YearRemodAdd` and `YrSold` are date features. Those are numerical features by default and they imply linear relationship. It might be better to use some of them as categorical features.\n",
        "\n",
        "There were **24** features with systematically missing data. Those features are filled with **None** and **0** depending on their types which made them sparse. Converting those features from multi class or continuous to binary, could give better results.\n",
        "\n",
        "Target (`SalePrice`) distribution is highly skewed and long tailed because of the outliers. It requires a transformation in order to perform better in models. Dealing with the outliers could also achieve better model performance.\n",
        "\n",
        "Many features are strongly correlated with each other and target. This relationship can be used to create new features with feature interaction in order to overcome multicollinearity issue.\n",
        "\n",
        "**12** continuous features are heavily stacked at **0**. They are also sparse features. Those features may add bias to the model. Some categorical features are not informative for two reasons. The feature is either too homogenous like `Utilities` feature, or all of the values have the same characteristics like `MoSold` feature. Those features can be conbined with other features or dropped completely.\n",
        "\n",
        "There are some numerical feature distributions that are too noisy. Their distributions in training and test set are quite different. They may require grouping to overcome this problem. The same problem occurs for categorical features as well. There are some categorical feature values that doesn't exist in both training set and test set. They need to be grouped with other values."
      ]
    },
    {
      "metadata": {
        "id": "dPVdjLyNnm_C"
      },
      "cell_type": "markdown",
      "source": [
        "## **2. Feature Engineering**"
      ]
    },
    {
      "metadata": {
        "id": "Iwwryg1Mnm_D"
      },
      "cell_type": "markdown",
      "source": [
        "### **2.1 Feature Interactions**\n",
        "Created **12** new features in this section. **6** of them are continuous, and **6** of them are categorical. `YearBuiltRemod`, `TotalSF`, `TotalSquare`, `TotalBath`,`TotalPorch` and `OverallRating` are the new continuous features. They are the total number of some related features. `HasPool`, `Has2ndFloor`, `HasGarage`, `HasBsmt` and `HasFireplace` are the new categorical features. They are the binary forms of some rare features. `NewHouse` is a categorical feature to separate houses which were built and sold at the same year."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "u0mnwkNinm_E"
      },
      "cell_type": "code",
      "source": [
        "df_all['YearBuiltRemod'] = df_all['YearBuilt'] + df_all['YearRemodAdd']\n",
        "df_all['TotalSF'] = df_all['TotalBsmtSF'] + df_all['1stFlrSF'] + df_all['2ndFlrSF']\n",
        "df_all['TotalSquareFootage'] = df_all['BsmtFinSF1'] + df_all['BsmtFinSF2'] + df_all['1stFlrSF'] + df_all['2ndFlrSF']\n",
        "df_all['TotalBath'] = df_all['FullBath'] + (0.5 * df_all['HalfBath']) + df_all['BsmtFullBath'] + (0.5 * df_all['BsmtHalfBath'])\n",
        "df_all['TotalPorchSF'] = df_all['OpenPorchSF'] + df_all['3SsnPorch'] + df_all['EnclosedPorch'] + df_all['ScreenPorch'] + df_all['WoodDeckSF']\n",
        "df_all['OverallRating'] = df_all['OverallQual'] + df_all['OverallCond']\n",
        "\n",
        "df_all['HasPool'] = df_all['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n",
        "df_all['Has2ndFloor'] = df_all['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n",
        "df_all['HasGarage'] = df_all['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n",
        "df_all['HasBsmt'] = df_all['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n",
        "df_all['HasFireplace'] = df_all['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "df_all['NewHouse'] = 0\n",
        "idx = df_all[df_all['YrSold'] == df_all['YearBuilt']].index\n",
        "df_all.loc[idx, 'NewHouse'] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Og9LFnv3nm_F"
      },
      "cell_type": "markdown",
      "source": [
        "### **2.2 Outliers**\n",
        "There are two houses larger than **4500** square feet and sold for less than **300000**. They are too large for their prices. Those two houses are dropped because they are affecting the regression coefficient of `GrLivArea` drastically. There is a house with less than **5** `OverallQual`, but sold for more than **200000**. That is an overly paid price for a low quality house, so it is dropped as well."
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "98nNEflAnm_F"
      },
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(12, 6))\n",
        "cmap = sns.color_palette('Set1', n_colors=10)\n",
        "\n",
        "sns.scatterplot(x=df_all['GrLivArea'], y='SalePrice', hue='OverallQual', palette=cmap, data=df_all)\n",
        "\n",
        "plt.xlabel('GrLivArea', size=15)\n",
        "plt.ylabel('SalePrice', size=15)\n",
        "plt.tick_params(axis='x', labelsize=12)\n",
        "plt.tick_params(axis='y', labelsize=12)\n",
        "\n",
        "plt.title('GrLivArea & OverallQual vs SalePrice', size=15, y=1.05)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "vXbxVft8nm_K"
      },
      "cell_type": "code",
      "source": [
        "df_all.drop(df_all[np.logical_and(df_all['OverallQual'] < 5, df_all['SalePrice'] > 200000)].index, inplace=True)\n",
        "df_all.drop(df_all[np.logical_and(df_all['GrLivArea'] > 4000, df_all['SalePrice'] < 300000)].index, inplace=True)\n",
        "df_all.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KDZTYAv3nm_L"
      },
      "cell_type": "markdown",
      "source": [
        "### **2.3 Encoding**"
      ]
    },
    {
      "metadata": {
        "id": "w7RtI0iknm_L"
      },
      "cell_type": "markdown",
      "source": [
        "#### **2.3.1 Label Encoding Ordinal Features**\n",
        "There are **45** categorical features, and **23** of them are ordinal. Different set of integers are mapped to those ordinal features depending on their values.\n",
        "\n",
        "**3** of the **23** ordinal features are dropped because they are not informative. Those features are `Street`, `Utilities` and `PoolQC`. The `PoolQC` and `Street` features have only different values in **10** houses, and `Utilities` feature has only **1** different value in one house. That's why those features don't provide any useful information.\n",
        "\n",
        "There are also partial ordinal features that are not label encoded because all of their values are not ordered."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ORN8ysBtnm_L"
      },
      "cell_type": "code",
      "source": [
        "bsmtcond_map = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4}\n",
        "bsmtexposure_map = {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}\n",
        "bsmtfintype_map = {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6}\n",
        "bsmtqual_map = {'None': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}\n",
        "centralair_map = {'Y': 1, 'N': 0}\n",
        "extercond_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
        "exterqual_map = {'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}\n",
        "fireplacequ_map = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
        "functional_map = {'Typ': 0, 'Min1': 1, 'Min2': 1, 'Mod': 2, 'Maj1': 3, 'Maj2': 3, 'Sev': 4}\n",
        "garagecond_map = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
        "garagefinish_map = {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}\n",
        "garagequal_map = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
        "heatingqc_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
        "kitchenqual_map = {'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}\n",
        "landslope_map = {'Gtl': 1, 'Mod': 2, 'Sev': 3}\n",
        "lotshape_map = {'Reg': 0, 'IR1': 1, 'IR2': 2, 'IR3': 3}\n",
        "paveddrive_map = {'N': 0, 'P': 1, 'Y': 2}\n",
        "\n",
        "df_all['BsmtCond'] = df_all['BsmtCond'].map(bsmtcond_map)\n",
        "df_all['BsmtExposure'] = df_all['BsmtExposure'].map(bsmtexposure_map)\n",
        "df_all['BsmtFinType1'] = df_all['BsmtFinType1'].map(bsmtfintype_map)\n",
        "df_all['BsmtFinType2'] = df_all['BsmtFinType2'].map(bsmtfintype_map)\n",
        "df_all['BsmtQual'] = df_all['BsmtQual'].map(bsmtqual_map)\n",
        "df_all['CentralAir'] = df_all['CentralAir'].map(centralair_map)\n",
        "df_all['ExterCond'] = df_all['ExterCond'].map(extercond_map)\n",
        "df_all['ExterQual'] = df_all['ExterQual'].map(exterqual_map)\n",
        "df_all['FireplaceQu'] = df_all['FireplaceQu'].map(fireplacequ_map)\n",
        "df_all['Functional'] = df_all['Functional'].map(functional_map)\n",
        "df_all['GarageCond'] = df_all['GarageCond'].map(garagecond_map)\n",
        "df_all['GarageFinish'] = df_all['GarageFinish'].map(garagefinish_map)\n",
        "df_all['GarageQual'] = df_all['GarageQual'].map(garagequal_map)\n",
        "df_all['HeatingQC'] = df_all['HeatingQC'].map(heatingqc_map)\n",
        "df_all['KitchenQual'] = df_all['KitchenQual'].map(kitchenqual_map)\n",
        "df_all['LandSlope'] = df_all['LandSlope'].map(landslope_map)\n",
        "df_all['LotShape'] = df_all['LotShape'].map(lotshape_map)\n",
        "df_all['PavedDrive'] = df_all['PavedDrive'].map(paveddrive_map)\n",
        "\n",
        "df_all.drop(columns=['Street', 'Utilities', 'PoolQC'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VW2CbBFsnm_W"
      },
      "cell_type": "markdown",
      "source": [
        "#### **2.3.2 One-Hot Encoding Nominal Features**\n",
        "The rest of the categorical features are nominal, and there are **25** of them. Those features are one-hot encoded because there is no order in their values. Partial ordinal features are also one-hot encoded along with nominal features."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "VDTPmnd1nm_X"
      },
      "cell_type": "code",
      "source": [
        "nominal_features = ['Alley', 'BldgType', 'Condition1', 'Condition2', 'Electrical',\n",
        "                    'Exterior1st', 'Exterior2nd', 'Fence', 'Foundation', 'GarageType',\n",
        "                    'Heating', 'HouseStyle', 'LandContour', 'LotConfig', 'MSSubClass',\n",
        "                    'MSZoning', 'MasVnrType', 'MiscFeature', 'MoSold', 'Neighborhood',\n",
        "                    'RoofMatl', 'RoofStyle', 'SaleCondition', 'SaleType', 'YrSold']\n",
        "\n",
        "encoded_features = []\n",
        "\n",
        "for feature in nominal_features:\n",
        "    encoded_df = pd.get_dummies(df_all[feature])\n",
        "    n = df_all[feature].nunique()\n",
        "    encoded_df.columns = ['{}_{}'.format(feature, col) for col in encoded_df.columns]\n",
        "    encoded_features.append(encoded_df)\n",
        "\n",
        "df_all = pd.concat([df_all, *encoded_features], axis=1)\n",
        "df_all.drop(columns=nominal_features, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q9jEshtZnm_Y"
      },
      "cell_type": "markdown",
      "source": [
        "### **2.4 Dealing with the Skewness**\n",
        "The skewed and long tailed distribution of the `SalePrice` is solved by applying **log(1 + x)** transformation. This transformation reduced skewness from **1.88** to **0.12** and reduced kurtosis from **6.53** to **0.80**. Target is normally distributed after this transformation. Probability is a straight line rather than a convex curve, which is an indicator of the reduced skewness."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "vnoy_FwZnm_Z"
      },
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(16, 16))\n",
        "plt.subplots_adjust(top=1.5, right=1.5)\n",
        "\n",
        "sns.distplot(df_all['SalePrice'].dropna(), hist=True, ax=axs[0][0])\n",
        "probplot(df_all['SalePrice'].dropna(), plot=axs[0][1])\n",
        "\n",
        "df_all['SalePrice'] = np.log1p(df_all['SalePrice'])\n",
        "\n",
        "sns.distplot(df_all['SalePrice'].dropna(), hist=True, ax=axs[1][0])\n",
        "probplot(df_all['SalePrice'].dropna(), plot=axs[1][1])\n",
        "\n",
        "axs[0][0].set_xlabel('Sale Price', size=20, labelpad=12.5)\n",
        "axs[1][0].set_xlabel('Sale Price', size=20, labelpad=12.5)\n",
        "axs[0][1].set_xlabel('Theoretical Quantiles', size=20, labelpad=12.5)\n",
        "axs[0][1].set_ylabel('Ordered Values', size=20)\n",
        "axs[1][1].set_xlabel('Theoretical Quantiles', size=20, labelpad=12.5)\n",
        "axs[1][1].set_ylabel('Ordered Values', size=20)\n",
        "\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        axs[i][j].tick_params(axis='x', labelsize=15)\n",
        "        axs[i][j].tick_params(axis='y', labelsize=15)\n",
        "\n",
        "axs[0][0].set_title('Distribution of Sale Price', size=25, y=1.05)\n",
        "axs[0][1].set_title('Sale Price Probability Plot', size=25, y=1.05)\n",
        "axs[1][0].set_title('Distribution of Sale Price After log1p Transformation', size=25, y=1.05)\n",
        "axs[1][1].set_title('Sale Price Probability Plot After log1p Transformation', size=25, y=1.05)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print('Training Set SalePrice Skew: {}'.format(df_all['SalePrice'].skew()))\n",
        "print('Training Set SalePrice Kurtosis: {}'.format(df_all['SalePrice'].kurt()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dd_Z5ZqUnm_a"
      },
      "cell_type": "markdown",
      "source": [
        "The other highly skewed features are also transformed, but **boxcox1p** transformation is used for those features. **boxcox1p** is defined as $((1 + x)^{\\lambda} - 1)$ if $\\lambda$ is not **0**, and $log(1 + x)$ if $\\lambda$ is **0**.\n",
        "\n",
        "**0.5** is used as a skewness threshold, and transformation is applied to features which have higher skew than the threshold. However, this doesn't work on every feature because some of them are sparse."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "BkFtzwf5nm_a"
      },
      "cell_type": "code",
      "source": [
        "cont_features = ['1stFlrSF', '2ndFlrSF', '3SsnPorch', 'BsmtFinSF1', 'BsmtFinSF2',\n",
        "                 'BsmtUnfSF', 'EnclosedPorch', 'GarageArea', 'GrLivArea', 'LotArea',\n",
        "                 'LotFrontage', 'LowQualFinSF', 'MasVnrArea', 'MiscVal', 'OpenPorchSF',\n",
        "                 'PoolArea', 'ScreenPorch', 'TotalBsmtSF', 'WoodDeckSF']\n",
        "\n",
        "skewed_features = {feature: df_all[feature].skew() for feature in cont_features if df_all[feature].skew() >= 0.5}\n",
        "transformed_skews = {}\n",
        "\n",
        "for feature in skewed_features.keys():\n",
        "    df_all[feature] = boxcox1p(df_all[feature], boxcox_normmax(df_all[feature] + 1))\n",
        "    transformed_skews[feature] = df_all[feature].skew()\n",
        "\n",
        "df_skew = pd.DataFrame(index=skewed_features.keys(), columns=['Skew', 'Skew after boxcox1p'])\n",
        "df_skew['Skew'] = skewed_features.values()\n",
        "df_skew['Skew after boxcox1p'] = transformed_skews.values()\n",
        "\n",
        "fig = plt.figure(figsize=(24, 12))\n",
        "\n",
        "sns.pointplot(x=df_skew.index, y='Skew', data=df_skew, markers=['o'], linestyles=['-'])\n",
        "sns.pointplot(x=df_skew.index, y='Skew after boxcox1p', data=df_skew, markers=['x'], linestyles=['--'], color='#bb3f3f')\n",
        "\n",
        "plt.xlabel('Skewed Features', size=20, labelpad=12.5)\n",
        "plt.ylabel('Skewness', size=20, labelpad=12.5)\n",
        "plt.tick_params(axis='x', labelsize=11)\n",
        "plt.tick_params(axis='y', labelsize=15)\n",
        "\n",
        "plt.title('Skewed Features Before and After boxcox1p Transformation', size=20)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QTg_zunpnm_b"
      },
      "cell_type": "markdown",
      "source": [
        "## **3. Models**"
      ]
    },
    {
      "metadata": {
        "id": "qlzVi-NXnm_b"
      },
      "cell_type": "markdown",
      "source": [
        "### **3.1 Feature Selection**\n",
        "Sparse features are dropped because they are tend to be ignored by tree algorithms. **99.94** is the threshold for the sparse features. If **99.94%** of the values of a feature are zeros, the feature is dropped. Other useless features like `Id` and `Dataset` are also dropped. Finally, `X_train`, `y_train` and `X_test` are separated and ready for the machine learning models."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "USgfAOAHnm_c"
      },
      "cell_type": "code",
      "source": [
        "sparse = []\n",
        "\n",
        "for feature in df_all.columns:\n",
        "    counts = df_all[feature].value_counts()\n",
        "    zeros = counts.iloc[0]\n",
        "    if zeros / len(df_all) * 100 > 99.94:\n",
        "        sparse.append(feature)\n",
        "\n",
        "df_all.drop(columns=sparse, inplace=True)\n",
        "\n",
        "df_train, df_test = df_all.loc[:1456], df_all.loc[1457:].drop(['SalePrice'], axis=1)\n",
        "drop_cols = ['Id', 'Dataset']\n",
        "X_train = df_train.drop(columns=drop_cols + ['SalePrice']).values\n",
        "y_train = df_train['SalePrice'].values\n",
        "X_test = df_test.drop(columns=drop_cols).values\n",
        "\n",
        "print('X_train shape: {}'.format(X_train.shape))\n",
        "print('y_train shape: {}'.format(y_train.shape))\n",
        "print('X_test shape: {}'.format(X_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GZnQC8yAnm_d"
      },
      "cell_type": "markdown",
      "source": [
        "### **3.2 Cost Function & Cross Validation**\n",
        "`rmse` calculates the root of **mean squared error**, and since the target variable is already at log space, this function calculates **root mean squared log error** which is the competition score metric.\n",
        "\n",
        "`cv_rmse` has to be implemented with `cross_val_score` function, which returns a vector of scores of the specified cost function (`rmse`) for every fold. Square root of that vector is the `rmse` of every fold. A **10** fold shuffled cross validation is used for validation sets."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "zUF3BRAXnm_d"
      },
      "cell_type": "code",
      "source": [
        "def rmse(y_train, y_pred):\n",
        "     return np.sqrt(mean_squared_error(y_train, y_pred))\n",
        "\n",
        "def cv_rmse(model, X=X_train, y=y_train):\n",
        "    return np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=kf))\n",
        "\n",
        "K = 10\n",
        "kf = KFold(n_splits=K, shuffle=True, random_state=SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8ZNMmMLynm_e"
      },
      "cell_type": "markdown",
      "source": [
        "### **3.3 Models & Stacking**\n",
        "Created **8** models and one of them is a stack of those models.\n",
        "* `RidgeCV`, `LassoCV` and `ElasticNetCV` are linear models with built-in cross validation\n",
        "* `SVR` is a linear support vector machine algorithm\n",
        "* `GradientBoostingRegressor`, `LGBMRegressor` and `XGBRegressor` are tree based regression model\n",
        "* Lastly, `StackingCVRegressor` is the stack of those models\n",
        "\n",
        "\n",
        "**Stacking** is an ensemble learning technique to combine multiple regression models via a meta-regressor to give improved prediction accuracy. In the standard stacking procedure, the first-level regressors are fit to the same training set that is used prepare the inputs for the second-level regressor, which may lead to overfitting. The `StackingCVRegressor`, however, uses the concept of **out-of-fold predictions** the dataset is split into k folds, and in k successive rounds, k-1 folds are used to fit the first level regressor. In each round, the first-level regressors are then applied to the remaining 1 subset that was not used for model fitting in each iteration. The resulting predictions are then stacked and provided -- as input data -- to the second-level regressor. After the training of the `StackingCVRegressor`, the first-level regressors are fit to the entire dataset for optimal predicitons.\n",
        "\n",
        "All of the models are stacked in `StackingCVRegressor`, and `XGBRegressor` is used as the meta-regressor.\n",
        "\n",
        "`use_features_in_secondary` parameter is set to **True**, which means that the meta-regressor will be trained both on the predictions of the original regressors and the original dataset. (If it is set to **False**, the meta-regressor will be trained only on the predictions of the original regressors.)\n",
        "\n",
        "![alt](https://i.ibb.co/71k3JR0/stacking-cv-regressor-overview.png)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "xJEtp7vHnm_e"
      },
      "cell_type": "code",
      "source": [
        "ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=np.arange(14.5, 15.6, 0.1), cv=kf))\n",
        "lasso = make_pipeline(RobustScaler(), LassoCV(alphas=np.arange(0.0001, 0.0009, 0.0001), random_state=SEED, cv=kf))\n",
        "elasticnet = make_pipeline(RobustScaler(), ElasticNetCV(alphas=np.arange(0.0001, 0.0008, 0.0001), l1_ratio=np.arange(0.8, 1, 0.025), cv=kf))\n",
        "svr = make_pipeline(RobustScaler(), SVR(C=20, epsilon=0.008, gamma=0.0003))\n",
        "gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.01, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state=SEED)\n",
        "lgbmr = LGBMRegressor(objective='regression',\n",
        "                      num_leaves=4,\n",
        "                      learning_rate=0.01,\n",
        "                      n_estimators=5000,\n",
        "                      max_bin=200,\n",
        "                      bagging_fraction=0.75,\n",
        "                      bagging_freq=5,\n",
        "                      bagging_seed=SEED,\n",
        "                      feature_fraction=0.2,\n",
        "                      feature_fraction_seed=SEED,\n",
        "                      verbose=0)\n",
        "xgbr = XGBRegressor(learning_rate=0.01,\n",
        "                    n_estimators=3500,\n",
        "                    max_depth=3,\n",
        "                    gamma=0.001,\n",
        "                    subsample=0.7,\n",
        "                    colsample_bytree=0.7,\n",
        "                    objective='reg:squarederror',\n",
        "                    nthread=-1,\n",
        "                    seed=SEED,\n",
        "                    reg_alpha=0.0001)\n",
        "stack = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, svr, gbr, lgbmr, xgbr), meta_regressor=xgbr, use_features_in_secondary=True)\n",
        "\n",
        "models = {'RidgeCV': ridge,\n",
        "          'LassoCV': lasso,\n",
        "          'ElasticNetCV': elasticnet,\n",
        "          'SupportVectorRegressor': svr,\n",
        "          'GradientBoostingRegressor': gbr,\n",
        "          'LightGBMRegressor': lgbmr,\n",
        "          'XGBoostRegressor': xgbr,\n",
        "          'StackingCVRegressor': stack}\n",
        "predictions = {}\n",
        "scores = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    start = datetime.now()\n",
        "    print('[{}] Running {}'.format(start, name))\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions[name] = np.expm1(model.predict(X_train))\n",
        "\n",
        "    score = cv_rmse(model, X=X_train, y=y_train)\n",
        "    scores[name] = (score.mean(), score.std())\n",
        "\n",
        "    end = datetime.now()\n",
        "\n",
        "    print('[{}] Finished Running {} in {:.2f}s'.format(end, name, (end - start).total_seconds()))\n",
        "    print('[{}] {} Mean RMSE: {:.6f} / Std: {:.6f}\\n'.format(datetime.now(), name, scores[name][0], scores[name][1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Xs1eEzAnm_f"
      },
      "cell_type": "markdown",
      "source": [
        "### **3.4 Evaluation & Blending**\n",
        "All of the models individually achieved scores between **0.10** and **0.12**, but when the predictions of those models are blended, they get **0.058**. That's because those models are actually overfitting to certain degree. They are very good at predicting a subset of houses, and they fail at predicting the rest of the dataset. When their predictions are blended, they complement each other."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "DSHA0Ewdnm_g"
      },
      "cell_type": "code",
      "source": [
        "def blend_predict(X):\n",
        "    return ((0.1 * elasticnet.predict(X)) +\n",
        "            (0.05 * lasso.predict(X)) +\n",
        "            (0.1 * ridge.predict(X)) +\n",
        "            (0.1 * svr.predict(X)) +\n",
        "            (0.1 * gbr.predict(X)) +\n",
        "            (0.15 * xgbr.predict(X)) +\n",
        "            (0.1 * lgbmr.predict(X)) +\n",
        "            (0.3 * stack.predict(X)))\n",
        "\n",
        "blended_score = rmse(y_train, blend_predict(X_train))\n",
        "print('Blended Prediction RMSE: {}'.format(blended_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "db1NqQ01nm_g"
      },
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(ncols=2, nrows=4, figsize=(18, 36))\n",
        "plt.subplots_adjust(top=1.5, right=1.5)\n",
        "\n",
        "for i, model in enumerate(models, 1):\n",
        "    plt.subplot(4, 2, i)\n",
        "    plt.scatter(predictions[model], np.expm1(y_train))\n",
        "    plt.plot([0, 800000], [0, 800000], '--r')\n",
        "\n",
        "    plt.xlabel('{} Predictions (y_pred)'.format(model), size=20)\n",
        "    plt.ylabel('Real Values (y_train)', size=20)\n",
        "    plt.tick_params(axis='x', labelsize=15)\n",
        "    plt.tick_params(axis='y', labelsize=15)\n",
        "\n",
        "    plt.title('{} Predictions vs Real Values'.format(model), size=25)\n",
        "    plt.text(0, 700000, 'Mean RMSE: {:.6f} / Std: {:.6f}'.format(scores[model][0], scores[model][1]), fontsize=25)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "ztIbduqanm_h"
      },
      "cell_type": "code",
      "source": [
        "scores['Blender'] = (blended_score, 0)\n",
        "\n",
        "fig = plt.figure(figsize=(24, 12))\n",
        "\n",
        "ax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\n",
        "for i, score in enumerate(scores.values()):\n",
        "    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='medium', color='black', weight='semibold')\n",
        "\n",
        "plt.xlabel('Model', size=20, labelpad=12.5)\n",
        "plt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\n",
        "plt.tick_params(axis='x', labelsize=11)\n",
        "plt.tick_params(axis='y', labelsize=12.5)\n",
        "\n",
        "plt.title('Scores of Models', size=20)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wvANObtPnm_j"
      },
      "cell_type": "markdown",
      "source": [
        "### **3.5 Submission**"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Za_KAVwAnm_j"
      },
      "cell_type": "code",
      "source": [
        "submission_df = pd.DataFrame(columns=['Id', 'SalePrice'])\n",
        "submission_df['Id'] = df_test['Id']\n",
        "submission_df['SalePrice'] = np.expm1(blend_predict(X_test))\n",
        "submission_df.to_csv('submissions.csv', header=True, index=False)\n",
        "submission_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "House Prices - Advanced Stacking Tutorial",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}